{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb4xHEte2SB8"
   },
   "source": [
    "# PREDICTIVE MODELS FOR AMAZON REVIEWS SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpXX4Cny2e-v"
   },
   "source": [
    "### About Amazon Reviews Dataset\n",
    "\n",
    "This dataset contains several million reviews of Amazon products, with the reviews separated into two classes for positive and negative reviews. The two classes are evenly balanced here.\n",
    "\n",
    "This is a large dataset, and the version that I am using here only has the text as a feature with no other metadata. This makes this an interesting dataset for doing NLP work. It is data written by users, so it's like that there are various typos, nonstandard spellings, and other variations that you may not find in curated sets of published text.\n",
    "\n",
    "In this notebook, I will do some very simple text processing and then try out two fairly unoptimized deep learning models:\n",
    "1. A convolutional neural net\n",
    "2. A recurrent neural net\n",
    "These models should achieve results that are within a couple percent of state of the art at predicting the binary sentiment of the reviews.\n",
    "\n",
    "To download the dataset: \n",
    "\n",
    "!kaggle kernels output muonneutrino/sentiment-analysis-with-amazon-reviews -p /path/to/dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Aji_nGEIfTez"
   },
   "outputs": [],
   "source": [
    "Kindly use GPU enabled machine for faster processing, I used Google Colab\n",
    "import os\n",
    "os.chdir('/content/drive/MyDrive/amazon-reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AzJV6TZ3fmwE",
    "outputId": "2080c7bd-a168-486a-adac-d07de2336152"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.ft.txt.bz2', 'train.ft.txt.bz2', 'amazon-reviews-sentiment-prediction.ipynb']\n"
     ]
    }
   ],
   "source": [
    "#import necessary packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import bz2\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"./\")) #check the datasets directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hztbqfRf4J5c"
   },
   "source": [
    "# Reading the text\n",
    "\n",
    "The text is held in a compressed format. Luckily, we can still read it line by line. The first word gives the label, so we have to convert that into a number and then take the rest to be the comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ulaIT83wfna2"
   },
   "outputs": [],
   "source": [
    "def get_labels_and_texts(file):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in bz2.BZ2File(file):\n",
    "        x = line.decode(\"utf-8\")\n",
    "        labels.append(int(x[9]) - 1)\n",
    "        texts.append(x[10:].strip())\n",
    "    return np.array(labels), texts\n",
    "train_labels, train_texts = get_labels_and_texts('train.ft.txt.bz2')\n",
    "test_labels, test_texts = get_labels_and_texts('test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aENAHF9H4rCI"
   },
   "source": [
    "# Text Preprocessing\n",
    "The first thing I'm going to do to process the text is to lowercase everything and then remove non-word characters. I replace these with spaces since most are going to be punctuation. Then I'm going to just remove any other characters (like letters with accents). It could be better to replace some of these with regular ascii characters but I'm just going to ignore that here. It also turns out if you look at the counts of the different characters that there are very few unusual characters in this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "271R-Pdj4pnv"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "NON_ALPHANUM = re.compile(r'[\\W]')\n",
    "NON_ASCII = re.compile(r'[^a-z0-1\\s]')\n",
    "def normalize_texts(texts):\n",
    "    normalized_texts = []\n",
    "    for text in texts:\n",
    "        lower = text.lower()\n",
    "        no_punctuation = NON_ALPHANUM.sub(r' ', lower)\n",
    "        no_non_ascii = NON_ASCII.sub(r'', no_punctuation)\n",
    "        normalized_texts.append(no_non_ascii)\n",
    "    return normalized_texts\n",
    "        \n",
    "train_texts = normalize_texts(train_texts)\n",
    "test_texts = normalize_texts(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbjiKuTJ4uf_"
   },
   "source": [
    "# Train/Validation Split\n",
    "Now I'm going to set aside 20% of the training set for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qenO25_lgQLm"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, random_state=57643892, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZbRw1q542ZR"
   },
   "source": [
    "Now we have to convert text to formats that will be used in deep learning. For this purpose, we are going to use Tokenizer with top 12000 words as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "wbBjGMnMgVCw"
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = 12000\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "train_texts = tokenizer.texts_to_sequences(train_texts)\n",
    "val_texts = tokenizer.texts_to_sequences(val_texts)\n",
    "test_texts = tokenizer.texts_to_sequences(test_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIoHhbS55Ot8"
   },
   "source": [
    "# Padding Sequences\n",
    "In order to use batches effectively, we're going to need to take my sequences and turn them into sequences of the same length i.e. length of the longest sentence in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xmNzdZ-GgWj0"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = max(len(train_ex) for train_ex in train_texts)\n",
    "train_texts = pad_sequences(train_texts, maxlen=MAX_LENGTH)\n",
    "val_texts = pad_sequences(val_texts, maxlen=MAX_LENGTH)\n",
    "test_texts = pad_sequences(test_texts, maxlen=MAX_LENGTH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJMg8IT85lg_"
   },
   "source": [
    "# Convolutional Neural Net Model\n",
    "This CNN has an embedding with a dimension of 64, 3 convolutional layers with the first two having batch normalization and max pooling and the last with global max pooling. The results are then passed to a dense layer and then the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "X-YlBRfLgXm9"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    sequences = layers.Input(shape=(MAX_LENGTH,))\n",
    "    embedded = layers.Embedding(MAX_FEATURES, 64)(sequences)\n",
    "    x = layers.Conv1D(64, 3, activation='relu')(embedded)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPool1D(3)(x)\n",
    "    x = layers.Conv1D(64, 5, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPool1D(5)(x)\n",
    "    x = layers.Conv1D(64, 5, activation='relu')(x)\n",
    "    x = layers.GlobalMaxPool1D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(100, activation='relu')(x)\n",
    "    predictions = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs=sequences, outputs=predictions)\n",
    "    model.compile(\n",
    "        optimizer='rmsprop',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVb61IVq5tIW"
   },
   "source": [
    "## Training the model on train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rTx8I5_qgYtS",
    "outputId": "add14543-86e8-47cb-933c-e8b98a71d54f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "22500/22500 [==============================] - 236s 10ms/step - loss: 0.1664 - binary_accuracy: 0.9368 - val_loss: 0.1476 - val_binary_accuracy: 0.9453\n",
      "Epoch 2/2\n",
      "22500/22500 [==============================] - 217s 10ms/step - loss: 0.1433 - binary_accuracy: 0.9474 - val_loss: 0.1511 - val_binary_accuracy: 0.9448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f87bd6a1b90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_texts, \n",
    "    train_labels, \n",
    "    batch_size=128,\n",
    "    epochs=2,\n",
    "    validation_data=(val_texts, val_labels), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQIGAzKC54t7"
   },
   "source": [
    "## Testing the model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54L-ThYAgZmD",
    "outputId": "c2948804-c855-42ee-a9ca-57bd49bccddd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9439\n",
      "F1 score: 0.9451\n",
      "ROC AUC score: 0.987\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(test_texts)\n",
    "print('Accuracy score: {:0.4}'.format(accuracy_score(test_labels, 1 * (preds > 0.5))))\n",
    "print('F1 score: {:0.4}'.format(f1_score(test_labels, 1 * (preds > 0.5))))\n",
    "print('ROC AUC score: {:0.4}'.format(roc_auc_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BR8SYuFZ5_Sa"
   },
   "source": [
    "## Saving the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "XdCGJlP51Y9N"
   },
   "outputs": [],
   "source": [
    "model.save_weights('./cnn_model_weights/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZeYF_E_6Jni"
   },
   "source": [
    "# Recurrent Neural Net Model\n",
    "For an RNN model I'm also going to use a simple model. This has an embedding, two GRU layers, followed by 2 dense layers and then the output layer. I'm using the CuDNNGRU rather than GRU because the former is supposed to run faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "cWncmCrpgcHV"
   },
   "outputs": [],
   "source": [
    "def build_rnn_model():\n",
    "    sequences = layers.Input(shape=(MAX_LENGTH,))\n",
    "    embedded = layers.Embedding(MAX_FEATURES, 64)(sequences)\n",
    "    x = tf.compat.v1.keras.layers.CuDNNGRU(128, return_sequences=True)(embedded)\n",
    "    x = tf.compat.v1.keras.layers.CuDNNGRU(128)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dense(100, activation='relu')(x)\n",
    "    predictions = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs=sequences, outputs=predictions)\n",
    "    model.compile(\n",
    "        optimizer='rmsprop',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "rnn_model = build_rnn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-MZOo2N6mkh"
   },
   "source": [
    "## Training the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "shaHj0vggdiu",
    "outputId": "c0cd22bc-fb3c-446a-ad30-9ecd880142ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22500/22500 [==============================] - 820s 36ms/step - loss: 0.1597 - binary_accuracy: 0.9393 - val_loss: 0.1335 - val_binary_accuracy: 0.9508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f87d9fa8d50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model.fit(\n",
    "    train_texts, \n",
    "    train_labels, \n",
    "    batch_size=128,\n",
    "    epochs=1,\n",
    "    validation_data=(val_texts, val_labels), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Srw_lrC36yBL"
   },
   "source": [
    "## Predicting on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fqtHhPtP6qV4",
    "outputId": "6a3238eb-aa4a-47ec-8f9d-d1bee9ea79c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.95\n",
      "F1 score: 0.9497\n",
      "ROC AUC score: 0.9881\n"
     ]
    }
   ],
   "source": [
    "preds = rnn_model.predict(test_texts)\n",
    "print('Accuracy score: {:0.4}'.format(accuracy_score(test_labels, 1 * (preds > 0.5))))\n",
    "print('F1 score: {:0.4}'.format(f1_score(test_labels, 1 * (preds > 0.5))))\n",
    "print('ROC AUC score: {:0.4}'.format(roc_auc_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlsNwvlU61K0"
   },
   "source": [
    "## Saving the weights of model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Bn3hmQ9Nge6G"
   },
   "outputs": [],
   "source": [
    "rnn_model.save_weights('./rnn_model_weights/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQ3d9IJ_1LoU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "amazon-reviews-sentiment-prediction.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
